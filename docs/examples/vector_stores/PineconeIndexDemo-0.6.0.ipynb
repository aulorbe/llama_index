{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a332ddd3",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/vector_stores/PineconeIndexDemo-0.6.0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307804a3-c02b-4a57-ac0d-172c30ddc851",
   "metadata": {},
   "source": [
    "# LlamaIndex + Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ba1864",
   "metadata": {},
   "source": [
    "In this tutorial, we show how to use LlamaIndex with Pinecone to answer complex queries over multiple data sources.  \n",
    "* While Pinecone provides a powerful and efficient retrieval engine,\n",
    "it remains challenging to answer complex questions that require multi-step reasoning and synthesis over many data sources.\n",
    "* With LlamaIndex, we combine the power of vector similiarty search and multi-step reasoning to delivery higher quality and richer responses.\n",
    "\n",
    "\n",
    "Here, we show 2 specific use-cases:\n",
    "1. compare and contrast queries over Wikipedia articles about different cities.\n",
    "2. temporal queries that require reasoning over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6915225",
   "metadata": {},
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b19510",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48af8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7010b1d-d1bb-4f08-9309-a328bb4ea396",
   "metadata": {},
   "source": [
    "#### Creating a Pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce3143d-198c-4dd2-8e5a-c5cdf94f017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pinecone-client==3.0.0.dev8\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fc463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "api_key = os.environ[\"PINECONE_API_KEY\"]\n",
    "\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3500dd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions are for text-embedding-ada-002\n",
    "\n",
    "pc.create_index(\n",
    "    name=\"quickstart\",\n",
    "    dimension=1536,\n",
    "    metric=\"euclidean\",\n",
    "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    ")\n",
    "\n",
    "# If you need to create a PodBased Pinecone index, you could alternatively do this:\n",
    "#\n",
    "# from pinecone import Pinecone, PodSpec\n",
    "#\n",
    "# pc = Pinecone(api_key='xxx')\n",
    "#\n",
    "# pc.create_index(\n",
    "# \t name='my-index',\n",
    "# \t dimension=1536,\n",
    "# \t metric='cosine',\n",
    "# \t spec=PodSpec(\n",
    "# \t\t environment='us-east1-gcp',\n",
    "# \t\t pod_type='p1.x1',\n",
    "# \t\t pods=1\n",
    "# \t )\n",
    "# )\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1544b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_index = pc.Index(\"quickstart-index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027b8cae",
   "metadata": {},
   "source": [
    "# Use-Case 1: Compare and Contrast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d56590",
   "metadata": {},
   "source": [
    "#### Load Dataset\n",
    "\n",
    "Fetch and load Wikipedia pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c410eec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583afcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_titles = [\n",
    "    \"Toronto\",\n",
    "    \"Seattle\",\n",
    "    \"San Francisco\",\n",
    "    \"Chicago\",\n",
    "    \"Boston\",\n",
    "    \"Washington, D.C.\",\n",
    "    \"Cambridge, Massachusetts\",\n",
    "    \"Houston\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540a39b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "data_path = Path(\"data_wiki\")\n",
    "\n",
    "for title in wiki_titles:\n",
    "    response = requests.get(\n",
    "        \"https://en.wikipedia.org/w/api.php\",\n",
    "        params={\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": title,\n",
    "            \"prop\": \"extracts\",\n",
    "            \"explaintext\": True,\n",
    "        },\n",
    "    ).json()\n",
    "    page = next(iter(response[\"query\"][\"pages\"].values()))\n",
    "    wiki_text = page[\"extract\"]\n",
    "\n",
    "    if not data_path.exists():\n",
    "        Path.mkdir(data_path)\n",
    "\n",
    "    with open(data_path / f\"{title}.txt\", \"w\") as fp:\n",
    "        fp.write(wiki_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b802084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all wiki documents\n",
    "city_docs = {}\n",
    "all_docs = []\n",
    "for wiki_title in wiki_titles:\n",
    "    city_docs[wiki_title] = SimpleDirectoryReader(\n",
    "        input_files=[data_path / f\"{wiki_title}.txt\"]\n",
    "    ).load_data()\n",
    "    all_docs.extend(city_docs[wiki_title])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee4473a-094f-4d0a-a825-e1213db07240",
   "metadata": {},
   "source": [
    "#### Build Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2bcc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1558b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build index for each city document\n",
    "city_indices = {}\n",
    "index_summaries = {}\n",
    "for wiki_title in wiki_titles:\n",
    "    print(f\"Building index for {wiki_title}\")\n",
    "    # create storage context\n",
    "    vector_store = PineconeVectorStore(\n",
    "        pinecone_index=pinecone_index, namespace=wiki_title\n",
    "    )\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    # build index\n",
    "    city_indices[wiki_title] = VectorStoreIndex.from_documents(\n",
    "        city_docs[wiki_title], storage_context=storage_context\n",
    "    )\n",
    "\n",
    "    # set summary text for city\n",
    "    index_summaries[wiki_title] = f\"Wikipedia articles about {wiki_title}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04304299-fc3e-40a0-8600-f50c3292767e",
   "metadata": {},
   "source": [
    "#### Build Graph Query Engine for Compare & Contrast Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35369eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.composability import ComposableGraph\n",
    "from llama_index.indices.keyword_table.simple_base import (\n",
    "    SimpleKeywordTableIndex,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedbb693-725f-478f-be26-fa7180ea38b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = ComposableGraph.from_indices(\n",
    "    SimpleKeywordTableIndex,\n",
    "    [index for _, index in city_indices.items()],\n",
    "    [summary for _, summary in index_summaries.items()],\n",
    "    max_keywords_per_chunk=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8ddcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.query.query_transform.base import (\n",
    "    DecomposeQueryTransform,\n",
    ")\n",
    "from llama_index.query_engine.transform_query_engine import (\n",
    "    TransformQueryEngine,\n",
    ")\n",
    "\n",
    "decompose_transform = DecomposeQueryTransform(verbose=True)\n",
    "\n",
    "custom_query_engines = {}\n",
    "for wiki_title in wiki_titles:\n",
    "    index = city_indices[wiki_title]\n",
    "    query_engine = index.as_query_engine()\n",
    "    query_engine = TransformQueryEngine(\n",
    "        query_engine,\n",
    "        query_transform=decompose_transform,\n",
    "        transform_extra_info={\"index_summary\": index_summaries[wiki_title]},\n",
    "    )\n",
    "    custom_query_engines[index.index_id] = query_engine\n",
    "\n",
    "custom_query_engines[graph.root_id] = graph.root_index.as_query_engine(\n",
    "    retriever_mode=\"simple\",\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78235fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with query decomposition in subindices\n",
    "query_engine = graph.as_query_engine(custom_query_engines=custom_query_engines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd3616c",
   "metadata": {},
   "source": [
    "#### Run Compare & Contrast Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f210a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\n",
    "    \"Compare and contrast the demographics in Seattle, Houston, and Toronto.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24db9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.response.pprint_utils import pprint_response\n",
    "\n",
    "pprint_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a23412f",
   "metadata": {},
   "source": [
    "# Use-Case 2: Temporal Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce276271",
   "metadata": {},
   "source": [
    "Temporal queries such as \"what happened after X\" is intuitive to humans, but can often confuse vector databases.  \n",
    "\n",
    "This is because the vector embedding will focus on the subject \"X\" rather than the imporant temporal cue. This results in irrelevant and misleading context that harms the final answer.  \n",
    "\n",
    "LlamaIndex solves this by explicitly maintainging node relationships and leverage LLM to automatically perform query expansion to find more relevant context.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3150f9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader, StorageContext, VectorStoreIndex\n",
    "from llama_index.vector_stores import PineconeVectorStore\n",
    "\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"../data/paul_graham\").load_data()\n",
    "\n",
    "# define storage context\n",
    "vector_store = PineconeVectorStore(\n",
    "    pinecone_index=pinecone_index, namespace=\"pg_essay_0.6.0\"\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# build index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    # override to store Node in document store in addition to vector store, necessary for the node postprocessor\n",
    "    store_nodes_override=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbe01aa",
   "metadata": {},
   "source": [
    "We can define an auto prev/next node postprocessor to leverage LLM reasoning to help query expansion (with relevant additional nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f2d0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor.node import (\n",
    "    AutoPrevNextNodePostprocessor,\n",
    ")\n",
    "\n",
    "# define postprocessor\n",
    "node_postprocessor = AutoPrevNextNodePostprocessor(\n",
    "    docstore=index.storage_context.docstore,\n",
    "    service_context=index.service_context,\n",
    "    num_nodes=3,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# define query engine\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=1,\n",
    "    node_postprocessors=[node_postprocessor],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56896a41",
   "metadata": {},
   "source": [
    "#### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd6c81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer that we need to search nodes after current one\n",
    "response = query_engine.query(\n",
    "    \"What did the author do after handing off Y Combinator to Sam Altman?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91a50f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.response.pprint_utils import pprint_response\n",
    "\n",
    "pprint_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22434b2e",
   "metadata": {},
   "source": [
    "In comparison, naive top-k retrieval results in irrelevant context and hallucinated answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93d2432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define query engine\n",
    "naive_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=1,\n",
    ")\n",
    "\n",
    "response = naive_query_engine.query(\n",
    "    \"What did the author do after handing off Y Combinator to Sam Altman?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab35766-5ff2-4c4f-98e6-81039d99cb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint_response(response, show_source=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64117464",
   "metadata": {},
   "source": [
    "#### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221001e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer that we need to search nodes before current one\n",
    "response = query_engine.query(\n",
    "    \"What did the author do before handing off Y Combinator to Sam Altman?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bf950f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint_response(response, show_source=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
